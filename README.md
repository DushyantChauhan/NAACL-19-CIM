# Contextual Inter-modal Attention for Multi-modal Sentiment Analysis
Contextual inter-modal attention for multi-modal sentiment analysis (https://www.aclweb.org/anthology/D18-1382/)

For the evaluation of our proposed MMMU approach, we employ two multi-modal benchmark datasets i.e, MOSEI and MOSI. You can download datasets from this link (https://drive.google.com/open?id=1IaErV0XIf8-F23wdOgjMaACYAELxneya).

first download the dataset from given link and set the path in the code accordingly.
make two folder (i) results and (ii) weights

how to run file:

For MOSEI dataset:
for trimodal-->>  python trimodal_MOSEI.py  

For MOSI dataset:
for trimodal-->>  python trimodal_MOSI.py  

========================

--versions--

python: 2.7

keras: 2.2.2

tensorflow: 1.9.0
